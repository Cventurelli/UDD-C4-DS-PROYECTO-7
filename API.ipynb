{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyq3EuWZqddfsDs4E7ZoIh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-XIjrMz9MfR",
        "outputId": "63a520cd-10ae-4831-d322-e1078f6c60d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aplicacion con API incluida"
      ],
      "metadata": {
        "id": "lgZ9sU3Y9hN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Configuraciones iniciales\n",
        "img_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Rutas de los modelos y datos\n",
        "model_dir = '/content/drive/MyDrive/BOOTCAMP_DATA_SCIENCE/PROYECTO_7'\n",
        "\n",
        "# Función para cargar modelos de manera segura\n",
        "def load_keras_model(path):\n",
        "    try:\n",
        "        model = load_model(path)\n",
        "        print(f\"Model loaded successfully from {path}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from {path}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Cargar modelos\n",
        "model_simple = load_keras_model(f'{model_dir}/final_model_simple.h5')\n",
        "model_cnn = load_keras_model(f'{model_dir}/final_model_cnn.h5')\n",
        "model_vgg16 = load_keras_model(f'{model_dir}/final_model_vgg16.h5')\n",
        "\n",
        "try:\n",
        "    meta_model = joblib.load(f'{model_dir}/saved_models/meta_model.h5')\n",
        "    print(\"Meta-model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load meta-model. Error: {e}\")\n",
        "    meta_model = None\n",
        "\n",
        "def make_predictions(file_path):\n",
        "    \"\"\"Procesa una imagen y usa los modelos para hacer una predicción.\"\"\"\n",
        "    try:\n",
        "        img = image.load_img(file_path, target_size=(img_size, img_size))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_batch = np.expand_dims(img_array, axis=0)\n",
        "        img_preprocessed = img_batch / 255.0\n",
        "\n",
        "        pred_simple = model_simple.predict(img_preprocessed) if model_simple else None\n",
        "        pred_cnn = model_cnn.predict(img_preprocessed) if model_cnn else None\n",
        "        pred_vgg16 = model_vgg16.predict(img_preprocessed) if model_vgg16 else None\n",
        "        stacked_features = np.concatenate([x for x in [pred_simple, pred_cnn, pred_vgg16] if x is not None], axis=1)\n",
        "\n",
        "        if meta_model:\n",
        "            final_prediction = meta_model.predict(stacked_features)\n",
        "            return 'Pneumonia' if final_prediction[0] == 1 else 'Normal'\n",
        "        else:\n",
        "            return \"Meta-model is not loaded.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during prediction: {e}\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Código de prueba para verificar la funcionalidad del módulo\n",
        "    test_image_path = '/content/drive/MyDrive/BOOTCAMP_DATA_SCIENCE/PROYECTO_7/DATA_CLEAN_BALANCED/val/pneumonia/person1946_bacteria_4874.jpeg'\n",
        "    try:\n",
        "        result = make_predictions(test_image_path)\n",
        "        print(f'Prediction for the test image is: {result}')\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XREhE8iSgnHR",
        "outputId": "953788d5-b33f-4866-e8bb-6da43db473b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/drive/MyDrive/BOOTCAMP_DATA_SCIENCE/PROYECTO_7/final_model_simple.h5\n",
            "Model loaded successfully from /content/drive/MyDrive/BOOTCAMP_DATA_SCIENCE/PROYECTO_7/final_model_cnn.h5\n",
            "Model loaded successfully from /content/drive/MyDrive/BOOTCAMP_DATA_SCIENCE/PROYECTO_7/final_model_vgg16.h5\n",
            "Meta-model loaded successfully.\n",
            "1/1 [==============================] - 1s 944ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2213a5a9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 169ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2213426320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 319ms/step\n",
            "Prediction for the test image is: An error occurred during prediction: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aplicación"
      ],
      "metadata": {
        "id": "uB6_BZk09mEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import joblib\n",
        "\n",
        "# Configuraciones iniciales\n",
        "img_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Rutas de los modelos y datos\n",
        "model_dir = '/content/drive/MyDrive/BOOTCAMP_DATA_SCIENCE/PROYECTO_7'\n",
        "\n",
        "# Cargar modelos\n",
        "model_simple = load_model(f'{model_dir}/final_model_simple.h5')\n",
        "model_cnn = load_model(f'{model_dir}/final_model_cnn.h5')\n",
        "model_vgg16 = load_model(f'{model_dir}/final_model_vgg16.h5')\n",
        "meta_model = joblib.load(f'{model_dir}/saved_models/meta_model.h5')\n",
        "\n",
        "def make_predictions(image_path):\n",
        "    \"\"\"Procesa una imagen y usa los modelos para hacer una predicción.\"\"\"\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    image = test_datagen.flow_from_directory(\n",
        "        image_path,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=1,\n",
        "        class_mode='binary',\n",
        "        shuffle=False\n",
        "    )\n",
        "    pred_simple = model_simple.predict(image)\n",
        "    pred_cnn = model_cnn.predict(image)\n",
        "    pred_vgg16 = model_vgg16.predict(image)\n",
        "    stacked_features = np.concatenate([pred_simple, pred_cnn, pred_vgg16], axis=1)\n",
        "    final_prediction = meta_model.predict(stacked_features)\n",
        "    return 'Pneumonia' if final_prediction[0] == 1 else 'Normal'\n"
      ],
      "metadata": {
        "id": "Wt0rdRo188hU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#API"
      ],
      "metadata": {
        "id": "PSf8778g9qHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ51nOzfOZKd",
        "outputId": "3e0228bc-f1a9-4519-b695-a03b13f1397c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.5)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yOYczQmQ0c5",
        "outputId": "3abeb63d-fa12-4f27-aac1-46645a2bbecf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(file_path):\n",
        "    \"\"\"Procesa una imagen y usa los modelos para hacer una predicción.\"\"\"\n",
        "    from tensorflow.keras.preprocessing import image\n",
        "    img = image.load_img(file_path, target_size=(img_size, img_size))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    img_preprocessed = img_batch / 255.0\n",
        "\n",
        "    pred_simple = model_simple.predict(img_preprocessed)\n",
        "    pred_cnn = model_cnn.predict(img_preprocessed)\n",
        "    pred_vgg16 = model_vgg16.predict(img_preprocessed)\n",
        "    stacked_features = np.concatenate([pred_simple, pred_cnn, pred_vgg16], axis=1)\n",
        "    final_prediction = meta_model.predict(stacked_features)\n",
        "    return 'Pneumonia' if final_prediction[0] == 1 else 'Normal'\n"
      ],
      "metadata": {
        "id": "ON6uqAReUo1k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Opcional: Configura tu token de ngrok (esto es recomendable para un uso más estable)\n",
        "# ngrok.set_auth_token(\"tu_token_de_ngrok\")\n",
        "\n",
        "# Opcional: Configura tu token de ngrok (esto es recomendable para un uso más estable)\n",
        "ngrok.set_auth_token(\"2ifROKcb0VRbpopQpwC4cyhnfhr_49M4Kf7F6kDdd4dhu1wsc\")\n",
        "\n",
        "# Detener todos los túneles existentes antes de crear uno nuevo\n",
        "ngrok.kill()\n",
        "\n",
        "# Establecer un túnel en el puerto 5000\n",
        "tunnel = ngrok.connect(5000)\n",
        "print(\"Ngrok Tunnel URL:\", tunnel.public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMJq28LxQ9K2",
        "outputId": "04607292-f006-40f0-bfd3-0736ffeface6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok Tunnel URL: https://dc3e-34-75-110-56.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ngrok.get_tunnels())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_nSOSbwRA3w",
        "outputId": "0fea7f81-a14d-4849-9d7b-657ff72b056e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<NgrokTunnel: \"https://dc3e-34-75-110-56.ngrok-free.app\" -> \"http://localhost:5000\">]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Aquí iría la lógica para manejar la solicitud POST\n",
        "    return jsonify({'message': 'Esta es la respuesta del endpoint /predict'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)  # Flask se ejecuta en el puerto 5000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwgsjJQr28JP",
        "outputId": "5e58a7b6-67fa-4b8d-9d8a-9c1907884a22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2024 04:21:31] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2024 04:21:31] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2024 04:21:43] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install flask-ngrok\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Aquí iría la lógica para manejar la solicitud POST\n",
        "    return jsonify({'message': 'Esta es la respuesta del endpoint /predict'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_with_ngrok(app)  # Use run_with_ngrok to expose the app\n",
        "    app.run()  # Flask will now be accessible via ngrok"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diqyLlXc_PMu",
        "outputId": "1825f001-8dec-4210-c7bc-a94490a0f310"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.5)\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "Exception in thread Thread-16:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 203, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 497, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 395, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 243, in connect\n",
            "    self.sock = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 218, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x782ca2c49690>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 515, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x782ca2c49690>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1378, in run\n",
            "    self.function(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 70, in start_ngrok\n",
            "    ngrok_address = _run_ngrok()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_ngrok.py\", line 35, in _run_ngrok\n",
            "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 519, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x782ca2c49690>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Configuraciones para la carga de archivos\n",
        "app.config['UPLOAD_FOLDER'] = '/tmp'  # Define una carpeta temporal para guardar los archivos\n",
        "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # Limita el tamaño del archivo a 16MB\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Verificar si el archivo parte existe en la solicitud\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file part'}), 400\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No selected file'}), 400\n",
        "    if file:\n",
        "        filename = secure_filename(file.filename)\n",
        "        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "        file.save(file_path)\n",
        "\n",
        "        # Usar la función de predicción con la ruta del archivo guardado\n",
        "        prediction = make_predictions(file_path)\n",
        "        return jsonify({'prediction': prediction})\n",
        "\n",
        "    return jsonify({'error': 'File not allowed'}), 400\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53-SIBFUgsF",
        "outputId": "9f5fcc74-8817-448a-fbc2-0ecd765a614d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}